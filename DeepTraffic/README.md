There are few questions that still need to be answered. 
* I understand that 
Deep Q learning equation: 
(input, lastReward) -> action. Network is taught to optimize cumulative reward.

* Why exactly does 'relu' not suffer from problem of vanishing gradient.
* What role does epsilon_min has to play in it ? 
* How does deepqlearn module works. 
* [Fun Experiment] How would you change the whole system, if it was a game of bumping cars. Car disappear from the game
 once they bump into our car. Objective is to maximise the number of cars we get bumped into.
  
